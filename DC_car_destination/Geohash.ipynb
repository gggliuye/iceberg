{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Geohash.geohash as geohash\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', low_memory=False)\n",
    "test_df = pd.read_csv('test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistance(latA, lonA, latB, lonB):  \n",
    "    ra = 6378140  # radius of equator: meter  \n",
    "    rb = 6356755  # radius of polar: meter  \n",
    "    flatten = (ra - rb) / ra  \t# Partial rate of the earth  \n",
    "    # change angle to radians  \n",
    "    radLatA = np.pi*(latA)/180\n",
    "    radLonA = np.pi*(lonA)/180  \n",
    "    radLatB = np.pi*(latB)/180  \n",
    "    radLonB = np.pi*(lonB)/180  \n",
    "    try: \n",
    "        pA = np.arctan(rb / ra * np.tan(radLatA))  \n",
    "        pB = np.arctan(rb / ra * np.tan(radLatB))  \n",
    "        x = np.arccos(np.sin(pA) * np.sin(pB) + np.cos(pA) * np.cos(pB) * np.cos(radLonA - radLonB))  \n",
    "        c1 = (np.sin(x) - x) * (np.sin(pA) + np.sin(pB))**2 / np.cos(x / 2)**2  \n",
    "        c2 = (np.sin(x) + x) * (np.sin(pA) - np.sin(pB))**2 / np.sin(x / 2)**2  \n",
    "        dr = flatten / 8 * (c1 - c2)  \n",
    "        distance = ra * (x + dr)  \n",
    "        return distance # meter  \n",
    "    except:\n",
    "        return 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = getDistance(train_df['start_lat'], train_df['start_lon'] , train_df['end_lat'], train_df['end_lon'])\n",
    "train_df['distance'] = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = [12,11,10,9,8,7,5,4,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoEncoding(data,precision=5):\n",
    "    # encode the lat and lon data using geohash\n",
    "    tmp = data[['start_lat','start_lon']]\n",
    "    geohashList = []\n",
    "    for i in tmp.values:\n",
    "        geohashList.append(geohash.encode(i[0],i[1],precision))\n",
    "    data['geohash{}'.format(precision)] = geohashList\n",
    "    return data\n",
    "\n",
    "def dateConvert(data,isTrain):\n",
    "    # print 'convert string to datetime'\n",
    "    data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "    # encoding start lat lon to geohash\n",
    "    for pres in precisions:\n",
    "        data = geoEncoding(data,pres)\n",
    "    if isTrain:\n",
    "        data['end_time'] = pd.to_datetime(data['end_time'])\n",
    "    data['weekday'] = data['start_time'].dt.weekday + 1\n",
    "    data['if_weekend'] = ( data['weekday'] >= 6 ).astype(int)\n",
    "    data['hour'] = data['start_time'].dt.hour\n",
    "    #data['morning_night'] = (data['hour']/4).astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dateConvert(train_df, True)\n",
    "test_df = dateConvert(test_df, False)\n",
    "train_df.to_csv('new_train.csv',index=False)\n",
    "test_df.to_csv('new_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('new_train.csv', low_memory=False)\n",
    "test_df = pd.read_csv('new_test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_key</th>\n",
       "      <th>out_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>geohash12</th>\n",
       "      <th>geohash11</th>\n",
       "      <th>geohash10</th>\n",
       "      <th>geohash9</th>\n",
       "      <th>geohash8</th>\n",
       "      <th>geohash7</th>\n",
       "      <th>geohash5</th>\n",
       "      <th>geohash4</th>\n",
       "      <th>geohash3</th>\n",
       "      <th>weekday</th>\n",
       "      <th>if_weekend</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SDK-XJ_609994b4d50a8a07a64d41d1f70bbb05</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-01-20 10:13:43</td>\n",
       "      <td>2018-01-20 10:19:04</td>\n",
       "      <td>33.783415</td>\n",
       "      <td>111.603660</td>\n",
       "      <td>33.779811</td>\n",
       "      <td>111.605885</td>\n",
       "      <td>wqp25w569v02</td>\n",
       "      <td>wqp25w569v0</td>\n",
       "      <td>wqp25w569v</td>\n",
       "      <td>wqp25w569</td>\n",
       "      <td>wqp25w56</td>\n",
       "      <td>wqp25w5</td>\n",
       "      <td>wqp25</td>\n",
       "      <td>wqp2</td>\n",
       "      <td>wqp</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDK-XJ_4c2f29d94c9478623711756e4ae34cc5</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-12 17:40:51</td>\n",
       "      <td>2018-02-12 17:58:13</td>\n",
       "      <td>34.810763</td>\n",
       "      <td>115.549264</td>\n",
       "      <td>34.814875</td>\n",
       "      <td>115.549374</td>\n",
       "      <td>ww4nj3h7mh81</td>\n",
       "      <td>ww4nj3h7mh8</td>\n",
       "      <td>ww4nj3h7mh</td>\n",
       "      <td>ww4nj3h7m</td>\n",
       "      <td>ww4nj3h7</td>\n",
       "      <td>ww4nj3h</td>\n",
       "      <td>ww4nj</td>\n",
       "      <td>ww4n</td>\n",
       "      <td>ww4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SDK-XJ_3570183177536a575b9da67a86efcd62</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-13 14:52:24</td>\n",
       "      <td>2018-02-13 15:24:33</td>\n",
       "      <td>34.640284</td>\n",
       "      <td>115.539024</td>\n",
       "      <td>34.813136</td>\n",
       "      <td>115.559243</td>\n",
       "      <td>ww4jj4hfq2uw</td>\n",
       "      <td>ww4jj4hfq2u</td>\n",
       "      <td>ww4jj4hfq2</td>\n",
       "      <td>ww4jj4hfq</td>\n",
       "      <td>ww4jj4hf</td>\n",
       "      <td>ww4jj4h</td>\n",
       "      <td>ww4jj</td>\n",
       "      <td>ww4j</td>\n",
       "      <td>ww4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SDK-XJ_78d749a376e190685716a51a6704010b</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-13 17:23:08</td>\n",
       "      <td>2018-02-13 17:39:02</td>\n",
       "      <td>34.818280</td>\n",
       "      <td>115.542039</td>\n",
       "      <td>34.813141</td>\n",
       "      <td>115.559217</td>\n",
       "      <td>ww4nj4rphtud</td>\n",
       "      <td>ww4nj4rphtu</td>\n",
       "      <td>ww4nj4rpht</td>\n",
       "      <td>ww4nj4rph</td>\n",
       "      <td>ww4nj4rp</td>\n",
       "      <td>ww4nj4r</td>\n",
       "      <td>ww4nj</td>\n",
       "      <td>ww4n</td>\n",
       "      <td>ww4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDK-XJ_3b249941c27834f5e43d43a9114e4909</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-13 18:06:02</td>\n",
       "      <td>2018-02-13 19:02:51</td>\n",
       "      <td>34.813278</td>\n",
       "      <td>115.559260</td>\n",
       "      <td>34.786126</td>\n",
       "      <td>115.874361</td>\n",
       "      <td>ww4nj9edjcms</td>\n",
       "      <td>ww4nj9edjcm</td>\n",
       "      <td>ww4nj9edjc</td>\n",
       "      <td>ww4nj9edj</td>\n",
       "      <td>ww4nj9ed</td>\n",
       "      <td>ww4nj9e</td>\n",
       "      <td>ww4nj</td>\n",
       "      <td>ww4n</td>\n",
       "      <td>ww4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     r_key          out_id  \\\n",
       "0  SDK-XJ_609994b4d50a8a07a64d41d1f70bbb05  2016061820000b   \n",
       "1  SDK-XJ_4c2f29d94c9478623711756e4ae34cc5  2016061820000b   \n",
       "2  SDK-XJ_3570183177536a575b9da67a86efcd62  2016061820000b   \n",
       "3  SDK-XJ_78d749a376e190685716a51a6704010b  2016061820000b   \n",
       "4  SDK-XJ_3b249941c27834f5e43d43a9114e4909  2016061820000b   \n",
       "\n",
       "           start_time            end_time  start_lat   start_lon    end_lat  \\\n",
       "0 2018-01-20 10:13:43 2018-01-20 10:19:04  33.783415  111.603660  33.779811   \n",
       "1 2018-02-12 17:40:51 2018-02-12 17:58:13  34.810763  115.549264  34.814875   \n",
       "2 2018-02-13 14:52:24 2018-02-13 15:24:33  34.640284  115.539024  34.813136   \n",
       "3 2018-02-13 17:23:08 2018-02-13 17:39:02  34.818280  115.542039  34.813141   \n",
       "4 2018-02-13 18:06:02 2018-02-13 19:02:51  34.813278  115.559260  34.786126   \n",
       "\n",
       "      end_lon     geohash12    geohash11   geohash10   geohash9  geohash8  \\\n",
       "0  111.605885  wqp25w569v02  wqp25w569v0  wqp25w569v  wqp25w569  wqp25w56   \n",
       "1  115.549374  ww4nj3h7mh81  ww4nj3h7mh8  ww4nj3h7mh  ww4nj3h7m  ww4nj3h7   \n",
       "2  115.559243  ww4jj4hfq2uw  ww4jj4hfq2u  ww4jj4hfq2  ww4jj4hfq  ww4jj4hf   \n",
       "3  115.559217  ww4nj4rphtud  ww4nj4rphtu  ww4nj4rpht  ww4nj4rph  ww4nj4rp   \n",
       "4  115.874361  ww4nj9edjcms  ww4nj9edjcm  ww4nj9edjc  ww4nj9edj  ww4nj9ed   \n",
       "\n",
       "  geohash7 geohash5 geohash4 geohash3  weekday  if_weekend  hour  \n",
       "0  wqp25w5    wqp25     wqp2      wqp        6           1    10  \n",
       "1  ww4nj3h    ww4nj     ww4n      ww4        1           0    17  \n",
       "2  ww4jj4h    ww4jj     ww4j      ww4        2           0    14  \n",
       "3  ww4nj4r    ww4nj     ww4n      ww4        2           0    17  \n",
       "4  ww4nj9e    ww4nj     ww4n      ww4        2           0    18  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruler geohash\n",
    "def ruler(train,test):\n",
    "    base = test\n",
    "    for c in precisions:\n",
    "        # case1: find the cases of the same car, same time, and similar geohash on weekday/weekend\n",
    "        tmp = train.groupby(['out_id', 'hour', 'if_weekend', 'geohash{}'.format(c)],as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_median{}'.format(c), 'end_lon': 'end_lon_median{}'.format(c)})\n",
    "        base = pd.merge(base, tmp,on=['out_id', 'hour','if_weekend', 'geohash{}'.format(c)], how='left', copy=False)\n",
    "        \n",
    "        # case2: find the cases of the same car, similar geohash, on weekday/weekend\n",
    "        tmp = train.groupby(['out_id', 'if_weekend' , 'geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_weekday{}'.format(c), 'end_lon': 'end_lon_weekday{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['out_id', 'if_weekend' , 'geohash{}'.format(c)], how='left', copy=False)\n",
    "\n",
    "        # case2': find the cases of the same car, similar geohash, on weekday/weekend\n",
    "        tmp = train.groupby(['out_id', 'geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_idex{}'.format(c), 'end_lon': 'end_lon_idex{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['out_id', 'geohash{}'.format(c)], how='left', copy=False)\n",
    "        \n",
    "        # case3: find the cases of similar time and similar geohash\n",
    "        tmp = train.groupby(['hour','geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_only{}'.format(c), 'end_lon': 'end_lon_only{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['hour','geohash{}'.format(c)], how='left', copy=False)\n",
    "        \n",
    "        # case4: find the cases of the same time and similar geohash\n",
    "        tmp = train.groupby(['morning_night','geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_large{}'.format(c), 'end_lon': 'end_lon_large{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['morning_night','geohash{}'.format(c)], how='left', copy=False)\n",
    "        \n",
    "\n",
    "    # if case1-12 does not exist, fill it with other values in case1\n",
    "    for c in precisions[0:6]:\n",
    "        base['end_lat_median_final'] = base['end_lat_median12'].fillna(base['end_lat_median{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median12'].fillna(base['end_lon_median{}'.format(c)])\n",
    "\n",
    "    # if case1 dose not exist, fill it with case3\n",
    "    for c in precisions[0:6]:\n",
    "        base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['end_lat_weekday{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['end_lon_weekday{}'.format(c)])\n",
    "    \n",
    "    for c in precisions:\n",
    "        base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['end_lat_idex{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['end_lon_idex{}'.format(c)])\n",
    "\n",
    "    # if case1 and case2 do not exist, fill it with case1\n",
    "    for c in precisions:\n",
    "        base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['end_lat_only{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['end_lon_only{}'.format(c)])\n",
    "\n",
    "    for c in precisions:\n",
    "        base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['end_lat_large{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['end_lon_large{}'.format(c)])\n",
    "    \n",
    "    print('Finally, there are ', base['end_lat_median_final'].isna().sum(), ' values have no value.')\n",
    "    # if nothing exist fill it with the start position\n",
    "    base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['start_lat'])\n",
    "    base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['start_lon'])\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruler(train,test):\n",
    "    base = test\n",
    "    for c in precisions:\n",
    "        # case1: find the cases of the same car, same time, and similar geohash\n",
    "        tmp = train.groupby(['out_id', 'hour','if_weekend','geohash{}'.format(c)],as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_median{}'.format(c), 'end_lon': 'end_lon_median{}'.format(c)})\n",
    "        base = pd.merge(base, tmp,on=['out_id', 'hour','if_weekend','geohash{}'.format(c)], how='left', copy=False)\n",
    "        \n",
    "        # case2: find the cases of the same car, similar geohash\n",
    "        tmp = train.groupby(['out_id', 'geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_{}'.format(c), 'end_lon': 'end_lon_{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['out_id', 'geohash{}'.format(c)], how='left', copy=False)\n",
    "\n",
    "        # case3: find the cases of the same time and similar geohash\n",
    "        tmp = train.groupby(['hour','geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_only{}'.format(c), 'end_lon': 'end_lon_only{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['hour','geohash{}'.format(c)], how='left', copy=False)\n",
    "\n",
    "    # if case1-12 does not exist, fill it with other values in case1\n",
    "    for c in precisions[0:6]:\n",
    "        base['end_lat_median_final'] = base['end_lat_median12'].fillna(base['end_lat_median{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median12'].fillna(base['end_lon_median{}'.format(c)])\n",
    "\n",
    "    # if case1 dose not exist, fill it with case3\n",
    "    for c in precisions:\n",
    "        base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['end_lat_{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['end_lon_{}'.format(c)])\n",
    "\n",
    "    # if case1 and case2 do not exist, fill it with case1\n",
    "    for c in precisions[0:8]:\n",
    "        base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['end_lat_only{}'.format(c)])\n",
    "        base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['end_lon_only{}'.format(c)])\n",
    "\n",
    "    print('Finally, there are ', base['end_lat_median_final'].isna().sum(), ' values have no value.')\n",
    "    # if nothing exist fill it with the start position\n",
    "    base['end_lat_median_final'] = base['end_lat_median_final'].fillna(base['start_lat'])\n",
    "    base['end_lon_median_final'] = base['end_lon_median_final'].fillna(base['start_lon'])\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally, there are  182  values have no value.\n"
     ]
    }
   ],
   "source": [
    "base = ruler(train_df,test_df)\n",
    "\n",
    "submit = base[['r_key','end_lat_median_final','end_lon_median_final']]\n",
    "submit.columns = ['r_key','end_lat','end_lon']\n",
    "submit.to_csv('./new_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n",
    "- 'out_id', 'hour','geohash{}'.format(c)\n",
    "- 'out_id', 'geohash{}'.format(c)\n",
    "- 'hour','geohash{}'.format(c)\n",
    "-  score: 0.508572\n",
    "\n",
    "### Test 2\n",
    "- 'out_id', 'hour', 'if_weekend', 'geohash{}'\n",
    "- 'out_id', 'if_weekend' , 'geohash{}'\n",
    "- 'out_id', 'geohash{}'\n",
    "- 'hour','geohash{}'\n",
    "- 'morning_night','geohash{}'\n",
    "- 4686 non values\n",
    "- score : 0.51\n",
    "\n",
    "### Test 3\n",
    "- precisions = [12,11,10,9,8,7,5,4,3]\n",
    "- 'out_id', 'hour','geohash{}'.format(c)\n",
    "- 'out_id', 'geohash{}'.format(c)\n",
    "- 'hour','geohash{}'.format(c)\n",
    "-  score: 0.508572"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.5 tf gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

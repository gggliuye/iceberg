{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Geohash.geohash as geohash\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from math import radians, atan, tan, sin, acos, cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistance(latA, lonA, latB, lonB):\n",
    "    # calcualte distance from lat and lon\n",
    "    ra = 6378140  # radius of equator: meter\n",
    "    rb = 6356755  # radius of polar: meter\n",
    "    flatten = (ra - rb) / ra  # Partial rate of the earth\n",
    "    # change angle to radians\n",
    "    radLatA = radians(latA)\n",
    "    radLonA = radians(lonA)\n",
    "    radLatB = radians(latB)\n",
    "    radLonB = radians(lonB)\n",
    "    try:\n",
    "        pA = atan(rb / ra * tan(radLatA))\n",
    "        pB = atan(rb / ra * tan(radLatB))\n",
    "        x = acos(sin(pA) * sin(pB) + cos(pA) * cos(pB) * cos(radLonA - radLonB))\n",
    "        c1 = (sin(x) - x) * (sin(pA) + sin(pB)) ** 2 / cos(x / 2) ** 2\n",
    "        c2 = (sin(x) + x) * (sin(pA) - sin(pB)) ** 2 / sin(x / 2) ** 2\n",
    "        dr = flatten / 8 * (c1 - c2)\n",
    "        distance = ra * (x + dr)\n",
    "        return distance  # meter\n",
    "    except:\n",
    "        return 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistanceFromDF(data):\n",
    "    # calcuate the error (distance)\n",
    "    tmp = data[['end_lat','end_lon','end_lat_median12','end_lon_median12']].astype(float)\n",
    "    error = []\n",
    "    for i in tmp.values:\n",
    "        # print i[0],i[1],i[2],i[3]\n",
    "        t = getDistance(i[0],i[1],i[2],i[3])\n",
    "        error.append(t)\n",
    "    print(np.sum(f(np.array(error))) / tmp.shape[0])\n",
    "\n",
    "def f(d):\n",
    "    # calculate the sccore\n",
    "    return 1 / (1 + np.exp(-(d-1000)/250))\n",
    "\n",
    "def geoEncoding(data,precision=5):\n",
    "    # encode the lat and lon data using geohash\n",
    "    tmp = data[['start_lat','start_lon']]\n",
    "    geohashList = []\n",
    "    for i in tmp.values:\n",
    "        geohashList.append(geohash.encode(i[0],i[1],precision))\n",
    "    data['geohash{}'.format(precision)] = geohashList\n",
    "    return data\n",
    "\n",
    "\n",
    "def dateConvert(data,isTrain):\n",
    "    # print 'convert string to datetime'\n",
    "    data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "    # encoding start lat lon to geohash\n",
    "    data = geoEncoding(data,12)\n",
    "    data = geoEncoding(data,11)\n",
    "    data = geoEncoding(data,10)\n",
    "    data = geoEncoding(data,9)\n",
    "    data = geoEncoding(data,8)\n",
    "    data = geoEncoding(data,7)\n",
    "    data = geoEncoding(data,6)\n",
    "    if isTrain:\n",
    "        data['end_time'] = pd.to_datetime(data['end_time'])\n",
    "    data['weekday'] = data['start_time'].dt.weekday + 1\n",
    "    data['hour'] = data['start_time'].dt.hour\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', low_memory=False)\n",
    "test_df = pd.read_csv('test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_key</th>\n",
       "      <th>out_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SDK-XJ_609994b4d50a8a07a64d41d1f70bbb05</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-01-20 10:13:43</td>\n",
       "      <td>2018-01-20 10:19:04</td>\n",
       "      <td>33.783415</td>\n",
       "      <td>111.603660</td>\n",
       "      <td>33.779811</td>\n",
       "      <td>111.605885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDK-XJ_4c2f29d94c9478623711756e4ae34cc5</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-12 17:40:51</td>\n",
       "      <td>2018-02-12 17:58:13</td>\n",
       "      <td>34.810763</td>\n",
       "      <td>115.549264</td>\n",
       "      <td>34.814875</td>\n",
       "      <td>115.549374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SDK-XJ_3570183177536a575b9da67a86efcd62</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-13 14:52:24</td>\n",
       "      <td>2018-02-13 15:24:33</td>\n",
       "      <td>34.640284</td>\n",
       "      <td>115.539024</td>\n",
       "      <td>34.813136</td>\n",
       "      <td>115.559243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SDK-XJ_78d749a376e190685716a51a6704010b</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-13 17:23:08</td>\n",
       "      <td>2018-02-13 17:39:02</td>\n",
       "      <td>34.818280</td>\n",
       "      <td>115.542039</td>\n",
       "      <td>34.813141</td>\n",
       "      <td>115.559217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDK-XJ_3b249941c27834f5e43d43a9114e4909</td>\n",
       "      <td>2016061820000b</td>\n",
       "      <td>2018-02-13 18:06:02</td>\n",
       "      <td>2018-02-13 19:02:51</td>\n",
       "      <td>34.813278</td>\n",
       "      <td>115.559260</td>\n",
       "      <td>34.786126</td>\n",
       "      <td>115.874361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     r_key          out_id  \\\n",
       "0  SDK-XJ_609994b4d50a8a07a64d41d1f70bbb05  2016061820000b   \n",
       "1  SDK-XJ_4c2f29d94c9478623711756e4ae34cc5  2016061820000b   \n",
       "2  SDK-XJ_3570183177536a575b9da67a86efcd62  2016061820000b   \n",
       "3  SDK-XJ_78d749a376e190685716a51a6704010b  2016061820000b   \n",
       "4  SDK-XJ_3b249941c27834f5e43d43a9114e4909  2016061820000b   \n",
       "\n",
       "            start_time             end_time  start_lat   start_lon    end_lat  \\\n",
       "0  2018-01-20 10:13:43  2018-01-20 10:19:04  33.783415  111.603660  33.779811   \n",
       "1  2018-02-12 17:40:51  2018-02-12 17:58:13  34.810763  115.549264  34.814875   \n",
       "2  2018-02-13 14:52:24  2018-02-13 15:24:33  34.640284  115.539024  34.813136   \n",
       "3  2018-02-13 17:23:08  2018-02-13 17:39:02  34.818280  115.542039  34.813141   \n",
       "4  2018-02-13 18:06:02  2018-02-13 19:02:51  34.813278  115.559260  34.786126   \n",
       "\n",
       "      end_lon  \n",
       "0  111.605885  \n",
       "1  115.549374  \n",
       "2  115.559243  \n",
       "3  115.559217  \n",
       "4  115.874361  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruler geohash\n",
    "def ruler(train,test):\n",
    "    base = test\n",
    "    for c in [12,11,10,9,8,7]:\n",
    "        # case1: find the cases of the same car, same time, and similar geohash\n",
    "        tmp = train.groupby(['out_id', 'hour','geohash{}'.format(c)],as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_median{}'.format(c), 'end_lon': 'end_lon_median{}'.format(c)})\n",
    "        base = pd.merge(base, tmp,on=['out_id', 'hour','geohash{}'.format(c)], how='left', copy=False)\n",
    "        \n",
    "        # case2: find the cases of the same car, similar geohash\n",
    "        tmp = train.groupby(['out_id', 'geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_{}'.format(c), 'end_lon': 'end_lon_{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['out_id', 'geohash{}'.format(c)], how='left', copy=False)\n",
    "\n",
    "        # case3: find the cases of the same time and similar geohash\n",
    "        tmp = train.groupby(['hour','geohash{}'.format(c)], as_index=False)[['end_lat', 'end_lon']].median().rename(\n",
    "            columns={'end_lat': 'end_lat_only{}'.format(c), 'end_lon': 'end_lon_only{}'.format(c)})\n",
    "        base = pd.merge(base, tmp, on=['hour','geohash{}'.format(c)], how='left', copy=False)\n",
    "\n",
    "    # if case1-12 does not exist, fill it with other values in case1\n",
    "    for c in [12,11,10,9,8,7]:\n",
    "        base['end_lat_median12'] = base['end_lat_median12'].fillna(base['end_lat_median{}'.format(c)])\n",
    "        base['end_lon_median12'] = base['end_lon_median12'].fillna(base['end_lon_median{}'.format(c)])\n",
    "\n",
    "    # if case1 dose not exist, fill it with case3\n",
    "    for c in [12, 11, 10, 9, 8, 7]:\n",
    "        base['end_lat_median12'] = base['end_lat_median12'].fillna(base['end_lat_{}'.format(c)])\n",
    "        base['end_lon_median12'] = base['end_lon_median12'].fillna(base['end_lon_{}'.format(c)])\n",
    "\n",
    "    # if case1 and case2 do not exist, fill it with case1\n",
    "    for c in [12, 11, 10, 9, 8, 7]:\n",
    "        base['end_lat_median12'] = base['end_lat_median12'].fillna(base['end_lat_only{}'.format(c)])\n",
    "        base['end_lon_median12'] = base['end_lon_median12'].fillna(base['end_lon_only{}'.format(c)])\n",
    "\n",
    "    # if nothing exist fill it with the start position\n",
    "    base['end_lat_median12'] = base['end_lat_median12'].fillna(base['start_lat'])\n",
    "    base['end_lon_median12'] = base['end_lon_median12'].fillna(base['start_lon'])\n",
    "    return base\n",
    "\n",
    "if os.path.exists('./cache/trainT.csv') and os.path.exists('./cache/trainS.csv') and os.path.exists('./cache/valT.csv') \n",
    "     and os.path.exists('./cache/test.csv'):\n",
    "    print('read from cache')\n",
    "    trainT = pd.read_csv('./cache/trainT.csv')\n",
    "    trainS = pd.read_csv('./cache/trainS.csv')\n",
    "    valT = pd.read_csv('./cache/valT.csv')\n",
    "    test = pd.read_csv('./cache/test.csv')\n",
    "else:\n",
    "    print('begin')\n",
    "    train = pd.read_csv('../data/train.csv')\n",
    "    trainT = train[train['start_time']<='2018-06-30 23:59:59']\n",
    "    valT = train[train['start_time']>'2018-06-30 23:59:59']\n",
    "    test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "    trainT = dateConvert(trainT,True)\n",
    "    trainS = dateConvert(train,True)\n",
    "    valT = dateConvert(valT,True)\n",
    "    test = dateConvert(test,False)\n",
    "\n",
    "    trainT.to_csv('./cache/trainT.csv',index=False)\n",
    "    trainS.to_csv('./cache/trainS.csv',index=False)\n",
    "    valT.to_csv('./cache/valT.csv',index=False)\n",
    "    test.to_csv('./cache/test.csv',index=False)\n",
    "\n",
    "base = ruler(trainT,valT)\n",
    "getDistanceFromDF(base)\n",
    "\n",
    "print('submit')\n",
    "base = ruler(trainT,test)\n",
    "\n",
    "submit = base[['r_key','end_lat_median12','end_lon_median12']]\n",
    "submit.columns = ['r_key','end_lat','end_lon']\n",
    "submit.to_csv('./new_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的方案，就是先统计用户星期最喜欢去的地方，之后对这些地方标记一下，如果未来真的去过，标记1否则0\n",
    "def dateConvert(data,isTrain):\n",
    "    # convert the data format\n",
    "    print 'convert string to datetime'\n",
    "    data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "    if isTrain:\n",
    "        data['end_time'] = pd.to_datetime(data['end_time'])\n",
    "    data['weekday'] = data['start_time'].dt.weekday + 1\n",
    "    return data\n",
    "\n",
    "def latitude_longitude_to_go(data,isTrain):\n",
    "    # take five numbers after '.'\n",
    "    tmp = data[['start_lat','start_lon']]\n",
    "    start_geohash = []\n",
    "    for t in tmp.values:\n",
    "        start_geohash.append(str(round(t[0],5)) + '_' + str(round(t[1],5)))\n",
    "    data['startGo'] = start_geohash\n",
    "\n",
    "    if isTrain:\n",
    "        tmp = data[['end_lat','end_lon']]\n",
    "        end_geohash = []\n",
    "        for t in tmp.values:\n",
    "            end_geohash.append(str(round(t[0],5))+ '_' + str(round(t[1],5)))\n",
    "        data['endGo'] = end_geohash\n",
    "    return data\n",
    "\n",
    "# 用户去过最多的三个地方\n",
    "def getMostTimesCandidate(candidate):\n",
    "    mostTimeCandidate = candidate[candidate['start_time']<='2018-06-30 23:59:59']\n",
    "    mostTimeCandidate = mostTimeCandidate[['out_id','endGo','end_lat','end_lon','weekday']]\n",
    "    mostTimeCandidate_3 = mostTimeCandidate.groupby(['out_id','endGo','weekday'],as_index=False)['endGo'].agg(\n",
    "        {'mostCandidateCount':'count'})\n",
    "    mostTimeCandidate_3.sort_values(['mostCandidateCount','out_id'],inplace=True,ascending=False)\n",
    "    mostTimeCandidate_3 = mostTimeCandidate_3.groupby(['out_id','weekday']).tail(7)\n",
    "    return mostTimeCandidate_3\n",
    "\n",
    "# 经纬度和 string 转化\n",
    "def geoHashToLatLoc(data):\n",
    "    tmp = data[['endGo']]\n",
    "    predict_end_lat = []\n",
    "    predict_end_lon = []\n",
    "    for i in tmp.values:\n",
    "        lats, lons = str(i[0]).split('_')\n",
    "        predict_end_lat.append(lats)\n",
    "        predict_end_lon.append(lons)\n",
    "    data['predict_end_lat'] = predict_end_lat\n",
    "    data['predict_end_lon'] = predict_end_lon\n",
    "    return data\n",
    "\n",
    "def calcGeoHasBetween(go1,go2):\n",
    "    latA, lonA = str(go1).split('_')\n",
    "    latB, lonB = str(go2).split('_')\n",
    "    distence = getDistance(float(latA), float(lonA), float(latB), float(lonB))\n",
    "    return distence\n",
    "\n",
    "# start to end distance\n",
    "def calcGeoHasBetweenMain(data):\n",
    "    distance = []\n",
    "    tmp = data[['endGo','startGo']]\n",
    "    for i in tmp.values:\n",
    "        distance.append(calcGeoHasBetween(i[0],i[1]) / 1000 )\n",
    "    data['distance'] = distance\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training and the test data set\n",
    "print 'begin'\n",
    "# 用1-6月去提取最常去的地方\n",
    "# 用7 月 7 月去训练\n",
    "train = pd.read_csv('train.csv')\n",
    "print train['start_time'].min(),train['start_time'].max()\n",
    "print train[train['start_time']>'2018-06-30 23:59:59'].shape\n",
    "print train[train['start_time']<='2018-06-30 23:59:59'].shape\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "print test['start_time'].min(),test['start_time'].max()\n",
    "print test.shape\n",
    "\n",
    "trainIndex = train.shape[0]\n",
    "testIndex = test.shape[0]\n",
    "\n",
    "print trainIndex,testIndex\n",
    "\n",
    "train = dateConvert(train,True)\n",
    "test = dateConvert(test,False)\n",
    "\n",
    "train = latitude_longitude_to_go(train,True)\n",
    "test = latitude_longitude_to_go(test,False)\n",
    "\n",
    "train.to_csv('train1.csv',index=False)\n",
    "test.to_csv('test1.csv',index=False)\n",
    "\n",
    "print '##############################################'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the most viewed places\n",
    "userMostTimes3loc = getMostTimesCandidate(train)\n",
    "# the validation set\n",
    "val = train[train['start_time']>'2018-06-30 23:59:59']\n",
    "val = val[['r_key','out_id','end_lat','end_lon','weekday','startGo','endGo','start_lat','start_lon']]\n",
    "val.rename(columns={'endGo':'trueEndGo'},inplace=True)\n",
    "val = pd.merge(val,userMostTimes3loc,on=['out_id','weekday'],how='left',copy=False)\n",
    "val['endGo'] = val['endGo'].fillna(val['startGo'])\n",
    "val['flag1'] = val['trueEndGo'] == val['endGo']\n",
    "val['flag1'] = val['flag1'].astype(int)\n",
    "val = calcGeoHasBetweenMain(val)\n",
    "# test data set\n",
    "test = test[['r_key','out_id','weekday','startGo','start_lat','start_lon']]\n",
    "test = pd.merge(test,userMostTimes3loc,on=['out_id','weekday'],how='left',copy=False)\n",
    "test['endGo'] = test['endGo'].fillna(test['startGo'])\n",
    "test = calcGeoHasBetweenMain(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regression to predict probability \n",
    "feature = ['start_lat','start_lon','weekday','distance','mostCandidateCount']\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print 'training'\n",
    "lr = LogisticRegression()\n",
    "lr.fit(val[feature].fillna(-1).values, val['flag1'].values)\n",
    "print 'predicting'\n",
    "pre = lr.predict_proba(val[feature].fillna(-1).values)[:,1]\n",
    "\n",
    "val_result = val[['r_key','endGo','end_lat','end_lon',]]\n",
    "val_result['predict'] = pre\n",
    "val_result = val_result.sort_values(['predict'],ascending=False)\n",
    "val_result = val_result.drop_duplicates(['r_key'])\n",
    "\n",
    "val = geoHashToLatLoc(val)\n",
    "print 'loss is :'\n",
    "getDistanceFromDF(val)\n",
    "\n",
    "subPre = lr.predict_proba(test[feature].fillna(-1).values)[:,1]\n",
    "test_result = test[['r_key','endGo']]\n",
    "test_result['predict'] = subPre\n",
    "\n",
    "test_result = test_result.sort_values(['predict'],ascending=False)\n",
    "test_result = test_result.drop_duplicates(['r_key'])\n",
    "test_result = geoHashToLatLoc(test_result)\n",
    "\n",
    "submit = test_result[['r_key','predict_end_lat','predict_end_lon']]\n",
    "submit.columns = ['r_key','end_lat','end_lon']\n",
    "submit.to_csv('./result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.5 tf gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
